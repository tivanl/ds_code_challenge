---
title: "J-PAL & CCT Data Engineer Code Challenge"
author: "Tivan"
date: "2025-08-20"
format: 
  html:
    self-contained: true
    toc: true
    toc_float: true
    toc-depth: 2
    toc-title: Contents
    toc-location: left
    css: ["docs/styles.css"]
  gfm: 
    output-file: README.md
number-sections: true
editor: source
editor_options: 
  chunk_output_type: console
execute: 
  echo: true
  eval: true
  warning: false
  message: false
  cache: false
code-block-bg: true
---

<div align="center">
<img src="img/jpal_logo.png" alt="J-PAL logo" width="200"/>
<img src="img/city_emblem.png" alt="City emblem" width="200"/>
</div>

# Introduction {#sec-intro}

The purpose of this document is to walk through the my solutions to the [Code Challenge](https://github.com/cityofcapetown/ds_code_challenge/tree/j-pal-data-engineer) for the Data Engineer position at J-PAL Africa. The solutions are divided into three sections, each with a corresponding script. @sec-extraction corresponds to `1_s3_select.R` which retrieves data from AWS, @sec-initial corresponds to `2_initial_data_transformation.R` which combines uses spatial joins to combine two datasets, and @sec-further corresponds to `3_further_data_transformation.R` which applies spatial filtering, adds additional data to enrich the dataset, and retrieves additional data where necessary. 

# Challenge Solutions

The packages below need to be installed to enable the code in the scripts. There are some functions that use `system()` to make calls directly to the operating system. The code was written using Linux. 

```{r}
# a vector containing the names of all the packages used in the document
pkg <- c(
  "paws",
  "httr2",
  "tictoc",
  "logger",
  "glue",
  "sf",
  "jsonlite",
  "tmap",
  "furrr",
  "parallel",
  "janitor",
  "osrm",
  "readODS",
  "tidyverse"
)
# vector indicating whether the packages in pkg are installed
filter_vec <- !pkg %in% installed.packages()[,"Package"]
# install those that have not been installed
if(sum(filter_vec) > 0) walk(pkg[filter_vec], ~install.packages(.x, Ncpus = 5))

library(paws)
library(httr2)
library(tictoc)
library(logger)
library(glue)
library(sf)
library(jsonlite)
library(tmap)
library(furrr)
library(parallel)
library(janitor)
library(glue)
library(osrm)
library(readODS)
library(tidyverse)
```

Besides the packages read in above, I also define custom function which can be found in `scripts/helpers`. I source them in the code chunk below.

```{r}
source("scripts/helpers/download_cpt_s3_data.R")
source("scripts/helpers/st_join_contains.R")
source("scripts/helpers/load_cpt_suburbs.R")
```


## Data extraction {#sec-extraction}

The code below sets up a client for the S3 service. Note that the following environment variables have to be set before running the code: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION`. The `s3` client's operations can be called using syntax like `s3$operation()`. 

```{r}
# set up the s3 connection (the credentials should be saved in  .Renviron)
s3 <- paws::s3(
  config = list(
    credentials = list(
      creds = list(
        access_key_id = Sys.getenv("AWS_ACCESS_KEY_ID"),
        secret_access_key = Sys.getenv("AWS_SECRET_ACCESS_KEY")
      )
    ),
    region = Sys.getenv("AWS_REGION")
  )
)
```

The code below uses AWS S3 Select to retrieve only resolution 8 polygons from `city_hex_polygons_8_10_filter_8.geojson`. 

```{r}
# specify the file information for extraction 
bucket_name <- "cct-ds-code-challenge-input-data"
file_name <- "city-hex-polygons-8-10.geojson"
# specify the query to use for selecting only resolution 8 polygons
query <- "SELECT s.* FROM s3object[*].features[*] s WHERE s.properties.resolution = 8"

# start timer for data retrieval operation
tic("Retrieving resolution 8 data from city_hex_polygons_8_10_filter_8.geojson took")
# use the select object content operation to retrieve the data from the file
hex_8_stream <- s3$select_object_content(
  Bucket = bucket_name,
  Key = file_name,
  Expression = query,
  ExpressionType = "SQL",
  InputSerialization = list(
    # geojsons are jsons so we can use the json format
    JSON = list(Type = "DOCUMENT"),
    CompressionType = "NONE"
  ),
  OutputSerialization = list(
    JSON = list(RecordDelimiter = "\n")
  )
)

# AWS S3 Select streams the response (because the size of the request is 
# unknown). The code below retrieves all the chunks from the response
hex_8_events <- hex_8_stream$Payload(
  function(x) {
    
    if (!is.null(x$Records)){
      output <- x$Records$Payload
      
      return(x)
      
    }
  }
)

# Define a function that can be used to loop over the events to extract text 
# from the raw response
extract_from_payload <- function(raw){
  
  if(is.raw(raw$Records$Payload)){
    return_char <- rawToChar(raw$Records$Payload)
  } else{
    return_char <- ""
  }
  
  return(return_char)
  
}

# extract all of the valid raw data that was streamed
map_chr(
  hex_8_events,
  extract_from_payload
) %>% 
  # collapse into a single string
  str_c(collapse = "") %>% 
  # remove trailing white space
  str_trim() %>% 
  # save the cleaned data in the appropriate file location
  writeLines(text = ., con = "data/city_hex_polygons_8_10_filter_8.geojson")

# end timer for the operation
toc()
```

The code below validates that the polygons retrieved using S3 Select match the data in `city_hex_polygons_8_10_filter_8.geojson`. 

```{r}
# read selected data in
cpt_hex_filtered_8 <- st_read("data/city_hex_polygons_8_10_filter_8.geojson")

# ensure the file to validate against is downloaded
tic("Time taken to download file (or check if already downloaded)")
hex_polygons_8_location <- "https://cct-ds-code-challenge-input-data.s3.af-south-1.amazonaws.com/city-hex-polygons-8.geojson" 
download_cpt_s3_data(hex_polygons_8_location)
toc()

# read the file in against which to validate
city_polygons_8 <- st_read("data/city-hex-polygons-8.geojson")

# validate that the two files are similar ---------------------------------

tic("Validation of city-hex-polygons-8-10 (res 8) against city-hex-polygons-8")
# get the number of rows in the larger file that are not contained in the smaller
# file
rows_8to10_not_in_8 <- cpt_hex_filtered_8 %>% 
  filter(resolution == 8) %>% 
  filter(!index %in% city_polygons_8$index) %>% 
  nrow()

# get the number of rows in the smaller file that are not contained in the larger
# file
rows_8_not_in_8to10 <- city_polygons_8 %>% 
  filter(!index %in% cpt_hex_filtered_8$index) %>% 
  nrow()

if(rows_8_not_in_8to10 == 0 & rows_8to10_not_in_8 == 0){
  log_info(
    "Exact match between city-hex-polygons-8 & city-hex-polygons-8-10 (res 8)"
  )
} else{
  log_info(
    glue("city-hex-polygons-8-10 (res 8) has {rows_8to10_not_in_8} rows that don't match city-hex-polygons-8")
  )
  log_info(
    glue("city-hex-polygons-8 has {rows_8_not_in_8to10} rows that don't match city-hex-polygons-8-10 (res 8)")
  )
}

# report the time consumed by the operation
toc()
```

## Initial Data Transformation {#sec-initial}

This section deals with the initial transformation of the service request data. The code chunk below uses a custom defined function (`download_cpt_s3_data`) which is sourced at the top of the document. It checks whether the relevant file exists locally and download it in case it does not exist. 

```{r}
# execute the download_cpt_s3_data function to retrieve the file if it doesn't 
# exist & provide the path if it does exist
tic("Time taken to retrieve sr_hex")
download_cpt_s3_data("https://cct-ds-code-challenge-input-data.s3.af-south-1.amazonaws.com/sr_hex.csv.gz")
download_cpt_s3_data("https://cct-ds-code-challenge-input-data.s3.af-south-1.amazonaws.com/sr.csv.gz")
toc()

```

## Further Data Transformation {#sec-further}


### Data anonymisation

There is an inherent trade off in securing data. Data that is completely useful is also completely insecure while data that is completely secure is completely useless. As an example, the most secure way to avoid data being used for something unwanted, might be to destroy all instances of the data, but that also means that nothing else can be done with the data. To balance usefulness and security, we want to get rid of the portions of the data that allow the end-user to be identified whilst maintaining as much useful information in the process. The `notification_number` is useful as the unique identifier (it might be useful if we need to link back to original data). To make it more secure, we use a hashing algorithm to pseudonymise it. I also added a prefix and suffix, and squared the number to make it more difficult to look it up via hash tables. As requested, the `creation_timestamp` is converted to 6-hour frequency (`creation_timestamp_6h`) and the location to 500m frequency (captured in the `geometry` column). The `completion_timestamp` is useful for determining the duration of the service issues, I calculate the duration of the issue in hours, keep the duration (`duration_to_completion_hours`) and drop the `completion_timestamp`. The wind direction and speed can be reverse engineered to determine the `creation_timestamp` to an hourly frequency, which defeats the purpose of the 6-hour frequency transformation. I therefore convert the wind data to hourly frequency and calculate the average direction (`wind_dir_deg`) and speed (`wind_speed_m_sec`) of the wind for each 6-hour interval before joining it to the service report data. I leave out the `reference_number` because it is not useful for analysis. I thought about iterating over the category variables (`directorate`, `department`, `branch`, `section`, `code_group`, `code`, `cause_code`) and change the value to `“Other”` in instances where there was only one occurrence of a value in the category but this felt like overkill. 
